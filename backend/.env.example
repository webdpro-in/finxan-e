# ============================================================================
# HARU AI TEACHER - BACKEND ENVIRONMENT CONFIGURATION
# ============================================================================
# This configuration file supports the Provider Abstraction Layer architecture,
# designed for 10-20 year longevity. The system uses a Contracts + Adapters
# pattern that allows switching between AI service providers without modifying
# application code. Simply change the provider environment variables below to
# switch between cloud APIs, local models, or future AI systems.
# ============================================================================

# ============================================================================
# PROVIDER SELECTION
# ============================================================================
# These variables control which provider implementation is used for each service.
# Provider switching is environment-based - change these values and restart the
# server to use different providers. No code changes required.

# AI Provider - Handles conversational AI and text generation
# Current options: gemini (PRIMARY), aws-bedrock
# Future options: openai, anthropic-direct, local-llama, cohere
# Default: gemini
AI_PROVIDER=gemini

# Text-to-Speech Provider - Converts text to natural speech audio
# Current options: aws-polly
# Future options: elevenlabs, google-tts, azure-tts, local-tts, coqui-tts
# Default: aws-polly
TTS_PROVIDER=aws-polly

# Speech-to-Text Provider - Transcribes audio to text
# Current options: aws-transcribe
# Future options: whisper, google-stt, azure-stt, deepgram, assembly-ai
# Default: aws-transcribe
STT_PROVIDER=aws-transcribe

# Image Provider - Searches and generates images
# Current options: aws-bedrock (uses Unsplash for search, placeholder for generation)
# Future options: stable-diffusion, dall-e, midjourney, local-sd, replicate
# Default: aws-bedrock
IMAGE_PROVIDER=aws-bedrock

# ============================================================================
# GOOGLE GEMINI CONFIGURATION
# ============================================================================
# Required when using Gemini as the AI provider (AI_PROVIDER=gemini)
# This is the CURRENT PRIMARY AI provider for the platform.

# Google Gemini API Key
# Get your key at: https://makersuite.google.com/app/apikey
# Required for: AI_PROVIDER=gemini
GEMINI_API_KEY=your_gemini_api_key_here

# Gemini Model Selection
# Available models: gemini-1.5-flash (fast, cost-effective), gemini-1.5-pro (advanced)
# Default: gemini-1.5-flash
GEMINI_MODEL=gemini-1.5-flash

# ============================================================================
# AWS CONFIGURATION
# ============================================================================
# Required when using any AWS provider (aws-bedrock, aws-polly, aws-transcribe)
# These credentials authenticate your application with AWS services.
# AWS is OPTIONAL - only needed if using AWS providers.

# AWS Region - Geographic region for AWS services
# Common values: us-east-1, us-west-2, eu-west-1, ap-northeast-1
AWS_REGION=us-east-1

# AWS Access Credentials
# Obtain these from AWS IAM console: https://console.aws.amazon.com/iam/
# Required permissions: Bedrock (InvokeModel), Polly (SynthesizeSpeech),
# Transcribe (StartTranscriptionJob, GetTranscriptionJob), S3 (PutObject, GetObject)
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_access_key_here

# S3 Bucket - Storage for audio files (TTS output and STT input)
# Create bucket in AWS S3 console: https://s3.console.aws.amazon.com/
# Bucket must allow public read access for TTS audio URLs
# Example: haru-ai-teacher-audio-production
S3_BUCKET_NAME=haru-ai-teacher-audio

# Bedrock Model ID - Specific AI model to use with AWS Bedrock
# Available models: https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html
# Recommended: anthropic.claude-3-sonnet-20240229-v1:0 (balanced performance/cost)
# Alternatives: anthropic.claude-3-opus-20240229-v1:0 (highest quality),
#               anthropic.claude-3-haiku-20240307-v1:0 (fastest/cheapest)
BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0

# ============================================================================
# EXTERNAL API KEYS
# ============================================================================
# Optional API keys for external services used by certain providers

# Unsplash API Key - For image search functionality
# Get your key at: https://unsplash.com/developers
# Used by: aws-bedrock image provider (search method)
# If not provided, placeholder images will be used
UNSPLASH_ACCESS_KEY=your_unsplash_access_key_here

# OpenAI API Key - For future OpenAI provider support
# Get your key at: https://platform.openai.com/api-keys
# Used by: openai provider (when AI_PROVIDER=openai)
# Not currently implemented - reserved for future use
OPENAI_API_KEY=your_openai_api_key_here

# ElevenLabs API Key - For future ElevenLabs TTS provider support
# Get your key at: https://elevenlabs.io/
# Used by: elevenlabs provider (when TTS_PROVIDER=elevenlabs)
# Not currently implemented - reserved for future use
ELEVENLABS_API_KEY=your_elevenlabs_api_key_here

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================

# Server Port - Port number for the backend API server
# Default: 3001 (frontend expects backend on this port)
PORT=3001

# Node Environment - Controls logging, error handling, and optimizations
# Values: development, production, test
# Use 'development' for local development with detailed logs
# Use 'production' for deployed environments with optimized performance
NODE_ENV=development

# ============================================================================
# PROVIDER LONGEVITY NOTES
# ============================================================================
# This architecture is designed to remain functional for 10-20 years by:
#
# 1. VENDOR INDEPENDENCE: Core application logic depends on stable contract
#    interfaces, not specific vendor implementations. If a provider shuts down
#    or changes their API, only the adapter needs updating.
#
# 2. ENVIRONMENT-BASED SWITCHING: Change providers by updating environment
#    variables - no code deployment required. This enables rapid response to
#    vendor issues, pricing changes, or service deprecations.
#
# 3. FUTURE-PROOF DESIGN: The contract interfaces are designed to accommodate
#    future AI capabilities (multimodal, streaming, fine-tuning) without
#    breaking existing code.
#
# 4. LOCAL MODEL SUPPORT: The architecture supports local/on-premises models
#    (e.g., local-llama, whisper, stable-diffusion) for air-gapped deployments
#    or privacy-sensitive use cases.
#
# 5. GRACEFUL DEGRADATION: If a provider fails, the system can fall back to
#    alternative providers or placeholder responses, ensuring continuous
#    operation even during vendor outages.
#
# To add a new provider:
# 1. Implement the appropriate contract interface (AIProvider, TTSProvider, etc.)
# 2. Add the provider to the registry factory method
# 3. Update this .env.example with the new provider option
# 4. No changes to routes, frontend, or core logic required
# ============================================================================
